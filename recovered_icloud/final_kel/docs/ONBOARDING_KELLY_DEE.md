# Onboarding: Kelly (Emotion) and Dee (Theory)

## Purpose
Quick orientation for new users: Kelly handles emotional inputs and intent; Dee handles music/theory generation.

## Kelly (Emotional Side)
- Role: Capture emotion/intent/consent; map feelings to musical targets.
- Inputs: Text/emotion tags/valence-arousal (and future biometric signals).
- Outputs: Emotional parameters (tempo targets, mode suggestions, rule-break hints, expression targets).
- How to use: Provide emotion text or valence/arousal; Kelly feeds intent into generation.

## Dee (Music/Theory Side)
- Role: Turn intent into music: harmony, melody, groove, arrangement, expression.
- Inputs: Kelly’s emotional parameters + user constraints (key/mode/genre/tempo).
- Outputs: Chords/voicings, melodies, grooves, bass, arrangements, dynamics.
- How to use: Set musical constraints (key/mode/genre), or accept Kelly’s suggestions.

## UI Tutorial (stub for future)
- Entry point: Home/Welcome → “Kelly & Dee” guide.
- Kelly panel: select emotion (text or wheel), see tempo/mode suggestions.
- Dee panel: preview chords/melody/groove/arrangement generated from Kelly’s intent.
- Toggle: accept Kelly’s defaults or override Dee’s musical constraints.

## Quick Steps
1) Set emotion with Kelly (text or valence/arousal). See suggested tempo/mode/rule breaks.
2) Let Dee generate music (chords/melody/groove/arrangement) with those targets.
3) Tweak constraints in Dee (key/mode/genre) or refresh from Kelly with a new emotion.

## Notes
- Naming is cosmetic: Kelly = emotional side, Dee = music/theory side. No behavior changes implied.
- Future: add UI walkthrough, screenshots, and examples once UI tutorial is built.
