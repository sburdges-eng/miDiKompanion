# Kelly ML/DSP Quick Start Guide

## ğŸš€ **Getting Started in 5 Minutes**

### **Option 1: Add Real-Time Emotion Recognition (Easiest)**

```bash
# 1. Install RTNeural
cd "/Users/seanburdges/Desktop/final kel"
git clone https://github.com/jatinchowdhury18/RTNeural.git external/RTNeural

# 2. Add to CMakeLists.txt
echo "add_subdirectory(external/RTNeural)" >> CMakeLists.txt
echo "target_link_libraries(KellyMidiCompanion PRIVATE RTNeural)" >> CMakeLists.txt

# 3. Create placeholder model
python3 -c "import json; json.dump({'layers': []}, open('emotion_model.json', 'w'))"

# 4. Rebuild
cmake --build build
```

**Time investment**: 2-3 weeks
**Benefit**: Real-time audio â†’ emotion detection

---

### **Option 2: Add AI MIDI Generation (Most Musical)**

```bash
# 1. Install PyTorch
pip3 install torch torchvision torchaudio

# 2. Prepare small dataset
mkdir -p ml_training/midi_data
# Copy 50-100 MIDI files to ml_training/midi_data

# 3. Train mini model
cd ml_training
python3 train_transformer.py --epochs 10 --batch-size 8

# 4. Export to ONNX
python3 export_transformer.py

# 5. Integrate with plugin (C++)
# See LEARNING_PROGRAM.md Module 3.1
```

**Time investment**: 4-6 weeks
**Benefit**: AI-generated melodies conditioned on emotion

---

### **Option 3: Add Timbre Transfer (Most Expressive)**

```bash
# 1. Install audio processing libs
pip3 install librosa soundfile

# 2. Collect audio samples
mkdir -p ml_training/audio_samples/violin
mkdir -p ml_training/audio_samples/voice
# Add 20-30 samples each

# 3. Train DDSP model
python3 ml_training/train_ddsp.py

# 4. Export for plugin
python3 ml_training/export_ddsp.py
```

**Time investment**: 6-8 weeks
**Benefit**: Neural synthesis with emotional timbre control

---

## ğŸ“Š **Current System Status**

### **âœ… What's Already Working**:
```
âœ“ 72-emotion PAD model (Pleasure-Arousal-Dominance)
âœ“ 5 emotion-to-music formulas (tempo, velocity, mode, reward, resonance)
âœ“ 14 MIDI generation engines (melody, bass, chords, pads, etc.)
âœ“ Full plugin build (AU + VST3)
âœ“ 29/29 unit tests passing
âœ“ Installed in Logic Pro
```

### **ğŸ¯ What We're Adding**:
```
â†’ Real-time neural emotion recognition from audio
â†’ AI-generated MIDI sequences (transformer)
â†’ Neural synthesis with timbre transfer (DDSP)
â†’ Desktop companion app for training models
â†’ Lock-free threading for real-time ML
```

---

## ğŸ§  **Architecture Overview**

```
CURRENT (Working):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Text Input   â”‚ â†’ WoundProcessor â†’ EmotionNode â†’ MidiGenerator â†’ MIDI Out
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â†“
                   PAD Coordinates (V/A/D)
                         â†“
                   Formulas (tempo/velocity/mode)

ENHANCED (Adding):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Audio Input  â”‚ â†’ Feature Extract â†’ RTNeural â†’ Emotion Vector â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                 â”‚
                                                                 â”œâ†’ Fusion
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                 â”‚
â”‚ Text Input   â”‚ â†’ WoundProcessor â†’ EmotionNode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â†“
                                    Enhanced VAD
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â†“                                         â†“
         Rule-Based Generator                    Transformer Generator
           (Fast, Therapeutic)                    (Creative, Varied)
                    â†“                                         â†“
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Merge â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â†“
                                 MIDI Notes
                                      â†“
                                 DDSP Voice
                                      â†“
                               Expressive Audio
```

---

## ğŸ“ **File Structure**

```
final kel/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ engine/
â”‚   â”‚   â”œâ”€â”€ EmotionMusicMapper.h          # âœ… Core formulas (done)
â”‚   â”‚   â”œâ”€â”€ NeuralEmotionProcessor.h      # ğŸ”„ Add RTNeural (new)
â”‚   â”‚   â””â”€â”€ TransformerMIDIGenerator.h    # ğŸ”„ Add transformer (new)
â”‚   â”œâ”€â”€ midi/
â”‚   â”‚   â””â”€â”€ MidiGenerator.cpp             # âœ… Main orchestrator (done)
â”‚   â”œâ”€â”€ voice/
â”‚   â”‚   â””â”€â”€ DDSPVoice.h                   # ğŸ”„ Add DDSP synth (new)
â”‚   â””â”€â”€ common/
â”‚       â””â”€â”€ LockFreeRingBuffer.h          # ğŸ”„ Add threading (new)
â”‚
â”œâ”€â”€ ml_training/                           # ğŸ”„ New folder
â”‚   â”œâ”€â”€ train_transformer.py
â”‚   â”œâ”€â”€ train_ddsp.py
â”‚   â”œâ”€â”€ export_transformer.py
â”‚   â””â”€â”€ datasets/
â”‚
â”œâ”€â”€ external/
â”‚   â””â”€â”€ RTNeural/                         # ğŸ”„ Clone from GitHub
â”‚
â”œâ”€â”€ LEARNING_PROGRAM.md                   # ğŸ“š Full curriculum
â””â”€â”€ QUICK_START_GUIDE.md                  # ğŸ“‹ This file
```

---

## ğŸ“ **Learning Path Decision Tree**

**Question 1**: Do you have MIDI files with emotion labels?
- **YES** â†’ Start with **Phase 3: Transformer** (best music quality)
- **NO** â†’ Start with **Phase 2: RTNeural** (works with any audio)

**Question 2**: Is your priority therapeutic accuracy or creative variety?
- **Therapeutic** â†’ Enhance current rule-based system with RTNeural
- **Creative** â†’ Add Transformer for AI-generated variations

**Question 3**: Do you need real-time synthesis or external DAW?
- **Real-time** â†’ Add **Phase 4: DDSP** voice
- **External** â†’ Just export MIDI (current system works)

---

## ğŸ’¡ **Recommended Starting Point**

### **For Most Users: Phase 2 (RTNeural)**

**Why?**
1. Fastest to implement (2-3 weeks)
2. Works with any audio input
3. Enhances existing system (no replacement needed)
4. Real-time performance

**What You Get:**
```
Before: Text â†’ Emotion â†’ MIDI
After:  Audio â†’ Neural Emotion Detection â†’ Enhanced MIDI
        Text  â†’        â†“
```

**First Steps:**
1. Read `LEARNING_PROGRAM.md` Module 1.1 (understand current system)
2. Read `LEARNING_PROGRAM.md` Module 2.1 (RTNeural integration)
3. Complete Exercise 2.1.1 (train simple model)
4. Test in plugin

---

## ğŸ”§ **Development Environment Setup**

```bash
# 1. Python environment
python3 -m venv venv
source venv/bin/activate
pip install torch torchvision torchaudio librosa soundfile onnx

# 2. C++ dependencies (already have JUCE)
brew install cmake ninja

# 3. Optional: ONNX Runtime (for transformer/DDSP)
brew install onnxruntime

# 4. Verify current build still works
cd "/Users/seanburdges/Desktop/final kel"
cmake --build build --target KellyTests
./build/tests/KellyTests
# Should see: [  PASSED  ] 29 tests
```

---

## ğŸ“ **Support & Resources**

### **Existing Documentation:**
- `EMOTION_TO_MUSIC_FORMULAS.md` - Current formula implementation
- `LEARNING_PROGRAM.md` - Complete ML/DSP curriculum
- `tests/` - 29 unit tests showing how everything works

### **External Resources:**
- **RTNeural**: https://github.com/jatinchowdhury18/RTNeural
- **DDSP**: https://github.com/magenta/ddsp
- **PyTorch**: https://pytorch.org/tutorials/
- **ONNX**: https://onnx.ai/

### **Your Codebase Highlights:**
- **Best starting point**: `src/engine/EmotionMusicMapper.h:34-45`
  - See how tempo/velocity formulas work
  - Add neural predictions alongside formulas

- **MIDI generation entry**: `src/midi/MidiGenerator.cpp:generate()`
  - Line ~38: Where emotions become music
  - Perfect place to insert transformer output

- **Plugin audio thread**: `src/plugin/PluginProcessor.cpp:processBlock()`
  - Where audio flows
  - Add feature extraction here

---

## âš¡ **Quick Wins**

### **Win #1: Add Emotion Smoothing (10 minutes)**

```cpp
// File: src/plugin/PluginProcessor.cpp
// Add after line ~50 (in processBlock)

// Smooth emotion updates
float alpha = 0.1f;  // Smoothing factor
currentValence = alpha * newValence + (1.0f - alpha) * currentValence;
currentArousal = alpha * newArousal + (1.0f - alpha) * currentArousal;

// Now use smoothed values for MIDI generation
```

**Effect**: Smoother emotion transitions, less jarring changes

---

### **Win #2: Add Emotion Visualization (30 minutes)**

```cpp
// File: src/plugin/PluginEditor.cpp
// Add to paint() method

void paint(juce::Graphics& g) override {
    // ... existing code ...

    // Draw emotion circle
    float x = (valence + 1.0f) * 0.5f * getWidth();   // -1 to 1 â†’ 0 to width
    float y = (1.0f - arousal) * getHeight();          // 0 to 1 â†’ height to 0

    g.setColour(juce::Colours::red);
    g.fillEllipse(x - 5, y - 5, 10, 10);
}
```

**Effect**: Real-time visualization of emotion state

---

### **Win #3: Log Emotion Stats (5 minutes)**

```cpp
// File: src/plugin/PluginProcessor.cpp

void processBlock(...) {
    // ... existing code ...

    static int logCounter = 0;
    if (++logCounter % 100 == 0) {  // Every 100 blocks
        DBG("Emotion: V=" << currentValence << " A=" << currentArousal
            << " â†’ Tempo=" << calculatedTempo << " BPM");
    }
}
```

**Effect**: See emotion-to-music mappings in console

---

## ğŸ¯ **Next Actions**

### **This Week:**
- [ ] Read `LEARNING_PROGRAM.md` Phase 1 (Foundation)
- [ ] Run all existing tests: `./build/tests/KellyTests`
- [ ] Trace one emotion through the system (Exercise 1.1.1)
- [ ] Decide which ML feature to add first

### **Next Week:**
- [ ] Set up Python environment
- [ ] Choose: RTNeural OR Transformer OR DDSP
- [ ] Complete Module 2.1, 3.1, or 4.1 from learning program
- [ ] Build first prototype

### **Month 1 Goal:**
- [ ] One ML feature fully integrated and working
- [ ] Can demonstrate emotion â†’ ML â†’ music pipeline
- [ ] Plugin still stable (all tests passing)

---

**Remember**: The current system already works beautifully! These ML additions are enhancements, not replacements. Start small, test often, and build incrementally.

**Need help?** Reference the detailed `LEARNING_PROGRAM.md` for step-by-step instructions on any module.
