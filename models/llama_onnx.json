{
  "model_name": "llama_onnx_placeholder",
  "model_path": "/absolute/path/to/llama.onnx",
  "provider": "cpu", 
  "session_threads": 4,
  "intra_op_num_threads": 4,
  "inter_op_num_threads": 1,
  "ep_options": {},
  "max_tokens": 128,
  "temperature": 0.8,
  "top_p": 0.9,
  "timeout_ms": 5000,
  "notes": "Replace model_path with your LLaMA ONNX file. Set provider to 'cpu', 'coreml', or 'cuda' as available."
}

