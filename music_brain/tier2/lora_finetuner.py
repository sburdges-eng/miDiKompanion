"""
Tier 2 LoRA Fine-tuner: Mac-friendly model adaptation.

Fine-tune pretrained models with LoRA adapters (Low-Rank Adaptation).
Reduces parameters from millions to thousands, fitting in 4-8GB RAM.

Key benefits for Mac:
  ✓ 10-100x fewer parameters to train
  ✓ Lower memory usage (fits M4 Pro 16GB)
  ✓ Faster training (2-4 hours vs 16+ hours)
  ✓ Preserves base model knowledge (transfer learning)
  ✓ Can merge back into base model for inference
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import json
import time
import logging

logger = logging.getLogger(__name__)


class Tier2LORAfinetuner:
    """
    Fine-tune pretrained models using LoRA adapters.

    Works with:
      - MelodyTransformer (melody generation)
      - GroovePredictor (timing/velocity)
      - HarmonyPredictor (chord progressions)
    """

    def __init__(
        self,
        base_model: nn.Module,
        model_name: str = "melody_transformer",
        device: str = "mps",
        lora_rank: int = 8,
        lora_alpha: float = 16.0,
        lora_dropout: float = 0.1,
        target_modules: Optional[List[str]] = None,
        verbose: bool = True
    ):
        """
        Initialize LoRA fine-tuner.

        Args:
            base_model: Pretrained model to fine-tune
            model_name: "melody_transformer", "groove_predictor", "harmony_predictor"
            device: "mps" (Mac), "cuda" (NVIDIA), "cpu"
            lora_rank: Adapter rank (4-16 typical; higher = more capacity)
            lora_alpha: Scaling factor (usually 2x rank)
            lora_dropout: Dropout in adapter layers
            target_modules: Which layers to apply LoRA to
            verbose: Enable logging
        """
        self.device = device
        self.model_name = model_name
        self.lora_rank = lora_rank
        self.lora_alpha = lora_alpha
        self.lora_dropout = lora_dropout
        self.verbose = verbose

        self.base_model = base_model.to(self.device)

        # Default target modules (attention projections)
        if target_modules is None:
            target_modules = ["q_proj", "v_proj", "fc1", "fc2"]
        self.target_modules = target_modules

        # Freeze base model
        for param in self.base_model.parameters():
            param.requires_grad = False

        # Apply LoRA
        self._apply_lora()
        self._log(f"✓ Initialized LoRA (rank={lora_rank}, alpha={lora_alpha})")

    def _log(self, msg: str):
        if self.verbose:
            logger.info(msg)

    def _apply_lora(self):
        """Apply LoRA adapters to target modules"""
        # Count trainable parameters before
        params_before = sum(p.numel() for p in self.base_model.parameters())

        # Wrap model with LoRA
        try:
            # Try using peft library if available
            from peft import get_peft_model, LoraConfig, TaskType

            lora_config = LoraConfig(
                task_type=TaskType.TOKEN_CLS,
                r=self.lora_rank,
                lora_alpha=self.lora_alpha,
                lora_dropout=self.lora_dropout,
                bias="none",
                target_modules=self.target_modules,
                modules_to_save=[]
            )

            self.model = get_peft_model(self.base_model, lora_config)

        except ImportError:
            # Fallback: Manual LoRA implementation
            self._log("⚠ peft not available; using manual LoRA implementation")
            self.model = self._manual_lora_wrap(self.base_model)

        # Count trainable parameters after
        params_after = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        self._log(f"  Base model: {params_before:,} parameters")
        self._log(f"  Trainable (LoRA): {params_after:,} parameters")
        self._log(f"  Reduction: {100 * (1 - params_after / params_before):.1f}%")

    def _manual_lora_wrap(self, model: nn.Module) -> nn.Module:
        """
        Manual LoRA implementation if peft is not available.

        Wraps Linear layers with low-rank adapters.
        """
        class LoRALinear(nn.Module):
            def __init__(self, original_layer: nn.Linear, rank: int, alpha: float):
                super().__init__()
                self.original = original_layer
                self.rank = rank
                self.alpha = alpha

                # LoRA matrices
                self.lora_a = nn.Linear(original_layer.in_features, rank, bias=False)
                self.lora_b = nn.Linear(rank, original_layer.out_features, bias=False)

                # Initialize
                nn.init.kaiming_uniform_(self.lora_a.weight, a=np.sqrt(5))
                nn.init.zeros_(self.lora_b.weight)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                # Original forward + LoRA update
                result = self.original(x)
                lora_update = self.lora_b(self.lora_a(x))
                result = result + (self.alpha / self.rank) * lora_update
                return result

        # Wrap all Linear layers
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                if any(target in name for target in self.target_modules):
                    # Replace with LoRA version
                    parent_name, child_name = name.rsplit(".", 1)
                    parent = dict(model.named_modules())[parent_name]
                    setattr(parent, child_name, LoRALinear(module, self.lora_rank, self.lora_alpha))

        return model

    def finetune_on_dataset(
        self,
        midi_paths: List[str],
        emotion_paths: List[str],
        epochs: int = 10,
        batch_size: int = 8,
        learning_rate: float = 1e-4,
        warmup_steps: int = 100,
        output_dir: str = "./checkpoints/tier2_lora",
        save_every_n_epochs: int = 2,
        validation_split: float = 0.1
    ) -> Dict:
        """
        Fine-tune model on custom dataset.

        Args:
            midi_paths: List of MIDI file paths
            emotion_paths: List of emotion embedding JSON paths
            epochs: Training epochs
            batch_size: Batch size (8-16 for M4 Pro)
            learning_rate: Learning rate for LoRA (lower than base)
            warmup_steps: Learning rate warmup steps
            output_dir: Where to save checkpoints
            save_every_n_epochs: Save frequency
            validation_split: Fraction for validation

        Returns:
            history: Training history dict
        """
        self._log(f"Preparing dataset: {len(midi_paths)} samples")

        # Create dataset
        dataset = MIDIEmotionDataset(midi_paths, emotion_paths, self.device)

        # Train/val split
        train_size = int(len(dataset) * (1 - validation_split))
        val_size = len(dataset) - train_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset,
            [train_size, val_size],
            generator=torch.Generator().manual_seed(42)
        )

        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)

        # Optimizer (use lower LR for LoRA)
        trainable_params = [p for p in self.model.parameters() if p.requires_grad]
        optimizer = torch.optim.AdamW(trainable_params, lr=learning_rate, weight_decay=0.01)

        # Learning rate scheduler
        scheduler = torch.optim.lr_scheduler.OneCycleLR(
            optimizer,
            max_lr=learning_rate,
            total_steps=len(train_loader) * epochs,
            pct_start=0.1
        )

        # Loss function
        loss_fn = nn.CrossEntropyLoss()

        # Training loop
        history = {
            "train_loss": [],
            "val_loss": [],
            "learning_rate": []
        }

        Path(output_dir).mkdir(parents=True, exist_ok=True)

        best_val_loss = float('inf')

        for epoch in range(epochs):
            # Training
            train_loss = self._train_epoch(
                train_loader, loss_fn, optimizer, scheduler
            )
            history["train_loss"].append(train_loss)
            history["learning_rate"].append(optimizer.param_groups[0]['lr'])

            # Validation
            val_loss = self._validate_epoch(val_loader, loss_fn)
            history["val_loss"].append(val_loss)

            self._log(f"Epoch {epoch+1}/{epochs}: "
                     f"train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, "
                     f"lr={optimizer.param_groups[0]['lr']:.2e}")

            # Save checkpoint
            if (epoch + 1) % save_every_n_epochs == 0:
                checkpoint_path = Path(output_dir) / f"epoch_{epoch+1}"
                checkpoint_path.mkdir(parents=True, exist_ok=True)
                self._save_checkpoint(checkpoint_path)
                self._log(f"  Saved checkpoint: {checkpoint_path}")

            # Early stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
            else:
                patience_counter += 1
                if patience_counter >= 3:
                    self._log(f"Early stopping at epoch {epoch+1}")
                    break

        # Save final
        final_path = Path(output_dir) / "final"
        final_path.mkdir(parents=True, exist_ok=True)
        self._save_checkpoint(final_path)
        self._log(f"✓ Training complete! Saved to {output_dir}")

        return history

    def _train_epoch(
        self,
        train_loader: DataLoader,
        loss_fn: nn.Module,
        optimizer: torch.optim.Optimizer,
        scheduler: torch.optim.lr_scheduler._LRScheduler
    ) -> float:
        """Train one epoch"""
        self.model.train()
        total_loss = 0.0

        for batch_idx, batch in enumerate(train_loader):
            # Get data
            notes = batch["notes"].to(self.device)
            emotion = batch["emotion"].to(self.device)

            # Forward
            outputs = self.model(emotion)  # (batch, seq_len, vocab)

            # Reshape for loss
            B, L, vocab_size = outputs.shape
            loss = loss_fn(
                outputs.reshape(B * L, vocab_size),
                notes.long().reshape(B * L)
            )

            # Backward
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()

            total_loss += loss.item()

        return total_loss / len(train_loader)

    def _validate_epoch(
        self,
        val_loader: DataLoader,
        loss_fn: nn.Module
    ) -> float:
        """Validate one epoch"""
        self.model.eval()
        total_loss = 0.0

        with torch.no_grad():
            for batch in val_loader:
                notes = batch["notes"].to(self.device)
                emotion = batch["emotion"].to(self.device)

                outputs = self.model(emotion)

                B, L, vocab_size = outputs.shape
                loss = loss_fn(
                    outputs.reshape(B * L, vocab_size),
                    notes.long().reshape(B * L)
                )

                total_loss += loss.item()

        return total_loss / len(val_loader)

    def _save_checkpoint(self, checkpoint_dir: Path):
        """Save LoRA weights"""
        try:
            # Try peft's save method
            self.model.save_pretrained(str(checkpoint_dir))
        except AttributeError:
            # Manual save
            checkpoint_dict = {}
            for name, param in self.model.named_parameters():
                if param.requires_grad:  # Only save LoRA params
                    checkpoint_dict[name] = param.detach().cpu()

            torch.save(checkpoint_dict, checkpoint_dir / "lora_weights.pt")

    def inference_with_lora(
        self,
        emotion_embedding: np.ndarray,
        length: int = 32,
        temperature: float = 0.9
    ) -> np.ndarray:
        """
        Generate using fine-tuned model.

        Args:
            emotion_embedding: (64,) emotion vector
            length: Number of notes to generate
            temperature: Sampling temperature

        Returns:
            notes: (length,) generated note indices
        """
        emotion_tensor = torch.FloatTensor(emotion_embedding).unsqueeze(0).to(self.device)

        self.model.eval()
        notes = []

        with torch.no_grad():
            context = emotion_tensor

            for step in range(length):
                outputs = self.model(context)
                next_logits = outputs[0, -1, :]

                # Temperature sampling
                next_logits = next_logits / max(temperature, 1e-6)
                probs = torch.softmax(next_logits, dim=-1)
                next_note = torch.multinomial(probs, 1).item()
                notes.append(next_note)

                # Update context (simplified)
                if step < length - 1:
                    note_encoding = torch.zeros(1, 1, 128, device=self.device)
                    note_encoding[0, 0, next_note] = 1.0
                    context = torch.cat([context[:, 1:, :], note_encoding], dim=1) \
                        if context.shape[1] > 1 else note_encoding

        return np.array(notes, dtype=np.int32)

    def merge_and_export(self, output_path: str):
        """
        Merge LoRA adapters back into base model for inference.

        Reduces model size and removes adapter overhead.
        Can be used with Tier 1 generators afterward.

        Args:
            output_path: Where to save merged model
        """
        try:
            merged_model = self.model.merge_and_unload()
            torch.save(merged_model.state_dict(), output_path)
            self._log(f"✓ Merged and exported to {output_path}")
        except AttributeError:
            self._log("⚠ Merge not available with manual LoRA; saving base model instead")
            torch.save(self.base_model.state_dict(), output_path)


class MIDIEmotionDataset(Dataset):
    """
    Dataset for MIDI + emotion pairs.

    Loads MIDI files and corresponding emotion embeddings.
    """

    def __init__(self, midi_paths: List[str], emotion_paths: List[str], device: str = "cpu"):
        self.midi_paths = midi_paths
        self.emotion_paths = emotion_paths
        self.device = device

    def __len__(self) -> int:
        return len(self.midi_paths)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        # Load MIDI as note sequence
        midi_notes = self._load_midi(self.midi_paths[idx])

        # Load emotion embedding
        with open(self.emotion_paths[idx]) as f:
            emotion = np.array(json.load(f), dtype=np.float32)

        return {
            "notes": torch.FloatTensor(midi_notes),
            "emotion": torch.FloatTensor(emotion)
        }

    def _load_midi(self, midi_path: str, max_length: int = 256) -> np.ndarray:
        """Load MIDI file as note sequence"""
        try:
            from music21 import converter

            try:
                score = converter.parse(midi_path)
                notes = [int(n.pitch.midi) for n in score.flatten().notes
                        if hasattr(n, 'pitch')]
            except:
                notes = []

        except ImportError:
            # Fallback: random notes
            notes = [np.random.randint(60, 72) for _ in range(max_length)]

        # Ensure fixed length
        notes = np.array(notes[:max_length], dtype=np.float32)
        if len(notes) < max_length:
            notes = np.pad(notes, (0, max_length - len(notes)), constant_values=0)

        return notes


# Convenience wrapper
def finetune_tier2(
    base_model: nn.Module,
    midi_paths: List[str],
    emotion_paths: List[str],
    device: str = "mps",
    epochs: int = 10,
    output_dir: str = "./checkpoints/tier2"
) -> Tier2LORAfinetuner:
    """
    Quick wrapper for Tier 2 fine-tuning.

    Example:
        finetuner = finetune_tier2(
            base_model, midi_paths, emotion_paths,
            device="mps", epochs=10
        )
    """
    finetuner = Tier2LORAfinetuner(base_model, device=device)
    finetuner.finetune_on_dataset(
        midi_paths, emotion_paths,
        epochs=epochs,
        output_dir=output_dir
    )
    return finetuner
